# Testing the bot

The bot has many moving parts and without tests it would be almost impossible to ensure that introducing some feature won't break something else. Even if the newly introduced feature is fully tried out manually, people aren't perfect and often don't realize that the change in 1 place could affect some code elsewhere, causing to problems in completely unrelated commands/functions. Since simply running the bot won't actually run these functions, but merely compile the files that they're in, many errors often go undiscovered by simply running the bot and testing the new feature.

Unit tests are the best way to avoid this problem. They simply run automated tests that do the intensive and repetitive work of checking and making sure that each function runs properly and wasn't affected by our changes.

## Running the tests

Our unit tests will be ran as a GitHub workflow, for every PR and every commit in the `main` branch. This ensures that all newly introduced changes are indeed properly implemented and aren't breaking existing functions.

Even though these tests will be ran automatically as workflows, this isn't the suggested way of testing whether your changes aren't breaking them, you should always run the unit tests locally, to avoid any unnecessary failing commits in PRs. To do this, you can simply use taskipy test task with poetry: `poetry run task test`.

## Mocks

We often need to pass discord.py specific classes, such as `discord.ext.commands.Context` or `discord.Member`, etc. as arguments for our functions (or any other objects from other libraries, discord.py is just the most common one), but these objects are usually obtained automatically from discord.py as the bot is working. Since we need to perform these tests independently of each other, we shouldn't rely on objects generated by external code. Relying on external code is bad, because then we wouldn't know if the test failure was caused by a failure on our side (something wrong with our function), or something external (such problem in discord.py library, or some other function that we use to generate this object). Since we can't use this external means of obtaining these objects, we instead "simulate" these objects with "fake" objects, that act similarly to the real ones, these objects are called "mocks".

These mocks come with the `unittest` library, specifically the [`unittest-mock`](https://docs.python.org/3/library/unittest.mock.html) module from python standard library.

Since our functions often rely on objects of same type, we have our commonly used custom definitions of these discord.py objects in [`tests/dpy_mocks.py`](/tests/dpy_mocks.py) that you can import and use from the individual test files.

Here is a simple example of a test that's using these mocks:
```py
import asyncio
import unittest

from bot.cogs.moderation.slowmode import Slowmode
from tests.dpy_mocks import MockBot, MockContext, MockTextChannel


class SlowmodeCogTests(unittest.TestCase):
    def setup(self) -> None:
        self.bot = MockBot()
        self.cog = Slowmode(self.bot)
        self.context = MockContext()

    def test_slowmode_change_sends_message(self):
        """
        Make sure that when the slow_mode command is ran properly, it sends
        the message we expect to the channel.
        """
        # Define new slowmode delay time that should be set for our text channel
        time = 10  # seconds

        # Create a mocked text channel to represent the channel that should have
        # it's slowmode delay changed by the slow_mode command
        text_channel = MockTextChannel(name="my-channel", slowmode_delay=1)

        # Set channel attribute for our mocked context, which is accessed from
        # context in the slow_mode command callback function
        self.context.channel = text_channel

        # Run the slow_mode command callback function (we use asyncio.run,
        # because we are in non-async function, so we can't use await here.)
        asyncio.run(self.cog.slow_mode.callback(self.cog, self.context, time))

        # Perform our assert check (this determines whether this test succeeded or failed)
        # this tests whether the context mock had `send` function ran with expected
        # message attribute
        self.context.send.assert_called_once_with(f"⏱️ Applied slowmode for this channel, time delay: {time} seconds.")

        # Sets the mock back to it's original state (without the knowledge that send method
        # has been called with any attributes)
        self.context.reset_mock()
```

## Changing/Writing Unit Tests

During the development of any new features, it is very common to change the way functions behave, or to remove or write new functions entirely. This will inevitably break the tests, or if it's just something new, it will decrease our test code coverage (the amount of tested code we have in the code-base), we always want to keep this coverage number as high as possible, so that every bit of code will be ran during testing. Because of this, you will likely often have to write new tests or change existing ones, and for that, you need a basic understanding of doing this.

For our testing, we use the [`unittest`](https://docs.python.org/3/library/unittest.html) module from python standard library.

When writing new tests, it is important to make sure that the test runs independently of any other objects. This means that the test you make won't influence any other tests, and also that the test won't fail because of some external problem, even though there's no actual problem in the code we're testing. You should always try to only test a single aspect of the code in a single test function, rather than making big test functions that test everything. This helps because we can very quickly identify which test case failed, and when it was only testing a single aspect, we immediately know that it's that aspect that's not working, whereas with a very big function that tests everything about given code, we only know that there is some aspect that failed within that tested code, but we don't immediately know what exactly has failed.

### Naming conventions

Unit testing is a very powerful way to quickly identify exactly what aspect of our tested code is failing, however this is only possible with a good naming of the testing methods and their docstrings. By giving a testing method a good and immediately understandable name that explains what it tests, we can instantly know what happened, whereas with a name like `test_lock_command` we only really know that there is some issue with the lock command, but nothing else, compare that to `test_lock_denies_send_permissions`, when a test like this would fail, we know that the lock command didn't actually change the permissions within a channel and didn't manage to actually lock a given channel.

However method names often aren't sufficient to precisely describe what are you trying to test exactly. Because of this, we can use docstrings to help us describe what's being tested in a more understandable and detailed way. We should try to keep this docstring on a single line, because `unittest` will actually display these docstrings when a test method fails, informing us what exactly went wrong.

### Independent tests with `self.subTest`

We often need to run more than one check with single test, we can have multiple test cases where it wouldn't really make sense to split them into multiple test methods. While we should always try and test only one aspect of the code with single test method, we also shouldn't make dozens of test methods for very little details, that will end up being very repetetive. When we encounter such scenario, we should instead use [`subTest`](https://docs.python.org/3/library/unittest.html#distinguishing-test-iterations-using-subtests) context manager. We often use this with a for loop, that's iterating through a list of our test cases, defined in some variable.

Using `subTest` is important, because it provides a way to separate multiple test cases within a single test method, and if we fail, we will actually see precisely which test case was being ran when we got the failure.

An example of using `unittest.TestCase.subTest`:
```py
    def test_duration_converter_valid_input(self):
        """Make sure that Duration converter returns expected amount of seconds for valid inputs."""
        test_values = (
            ("1y", 31536000),
            ("1mo", 2678400),
            ("1w", 604800),
            ("1d", 86400),
            ("1h", 3600),
            ("1m", 60),
            ("1s", 1)
        )

        converter = Duration()

        for duration_string, expected_seconds in test_values:
            with self.subTest(duration_string=duration_string, expected_seconds=expected_seconds):
                conversion = asyncio.run(converter.convert(self.context, duration_string))
                self.assertEqual(conversion, expected_seconds)
```

## Testing asynchronous functions

You might've noticed that in all of the above examples, the asynchronous functions were tested using `result = asyncio.run(coroutine)`, this is done because we've only used the basic synchronous `unittest.TestCase`, this class is used in most projects to test their synchronous functions, however testing async functions with it is a bit impractical, and for that reason, for the majority of the tests instead of using the regular `unittest.TestCase`, we are using [`unittest.IsolatedAsyncioTestCase`](https://docs.python.org/3/library/unittest.html#unittest.IsolatedAsyncioTestCase), this class provides us with the same functionality as the `unittest.TestCase` would, but on top of that, it also allows us to use async test methods. It also introduces `asyncSetUp` and `asyncTearDown` methods on top of the synchronous `setUp` and `tearDown` available in `unittest.TestCase`.

Here is a simple example of using this class based on an example above, that was using the synchronous version:
```py
import unittest

from bot.cogs.moderation.slowmode import Slowmode
from tests.dpy_mocks import MockBot, MockContext, MockTextChannel


class SlowmodeCogTests(unittest.IsolatedAsyncioTestCase):
    def setup(self) -> None:
        self.bot = MockBot()
        self.cog = Slowmode(self.bot)
        self.context = MockContext()

    async def test_slowmode_change_sends_message(self):
        """
        Make sure that when the slow_mode command is ran properly, it sends
        the message we expect to the channel.
        """
        time = 10  # seconds

        text_channel = MockTextChannel(name="my-channel", slowmode_delay=1)
        self.context.channel = text_channel

        # Await the async slow_mode command directly instead of using asyncio.run
        await self.cog.slow_mode(self.cog, self.context, time)

        self.context.send.assert_called_once_with(f"⏱️ Applied slowmode for this channel, time delay: {time} seconds.")
        self.context.reset_mock()
```

## Importance of test coverage

When writing tests, our main consideration is to increase the code coverage and ensure that ideally, every line of our tested function/class/code part was ran and tested. This is important because python won't report most errors on compile time (when the file is imported) but rather on runtime (when the line of code is actually ran), which means running all lines of code in the tested element ensures that there aren't any such errors that would otherwise go undetected.

However test coverage isn't everything. This is important to remember, even if the code has 100% code coverage, that doesn't mean it's fully tested nor guaranteed to work. Simply because just running the code doesn't mean we have tested it properly, for example if there is `int(x)` function in the code, and we only tested the function with `x="4"`, this will succeed, however we haven't actually tried to run `x="hi"`, which would raise an exception and our code would fail. Cases like these are quite common because we often don't think to test something so unexpected, however we should always try to test all possible aspects that could occur, even if we've already reached 100% code coverage.

## Unit Testing vs Integration Testing

Another restriction to think about when blindly attempting that the code works because our tests run is that, as the name would imply, it was only tested in separate "units", i.e. we only tested separate functions, which means we ensure they work independently, this however doesn't mean that when we actually run our code it will all work together. We should also acknowledge that mocking gives us a lot of flexibility that the real objects may not provide. Mocked objects will usually have any attribute accessible, returning another mock, which means if a function were to access something that shouldn't really be available, but the mock provided it, it would mean that when we actually run our code, it will fail.

Because this project currently doesn't have any automated integration testing, **it's very important that the bot is still actually ran and tested manually** in addition to running the unit-tests.
